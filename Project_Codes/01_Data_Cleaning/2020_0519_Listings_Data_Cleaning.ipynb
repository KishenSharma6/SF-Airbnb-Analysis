{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In the following notebook, I will be cleaning a raw data file of listings data from Inside Airbnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set notebook preferences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set pandas preferences\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "#Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path to data on local machine\n",
    "path = r'C:\\Users\\kishe\\Documents\\Data Science\\Projects\\Python\\In Progress\\Airbnb - San Francisco\\Data\\01_Raw\\SF Airbnb'\n",
    "\n",
    "#Read in data\n",
    "df = pd.read_csv(path + '/2020_0519_Aggregated_Listings.csv',parse_dates= ['host_since','last_review', 'first_review'],index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preview Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display data, print shape\n",
    "print('Data shape:', df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View data description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View data description\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop mostly homogenous/redundant columns and columns with only missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract cols with values with more than 1 unique value\n",
    "df = df.loc[:,(df.nunique() != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing columns\n",
    "df.dropna(axis =1,how = 'all', inplace = True)\n",
    "\n",
    "#Drop redundant columns\n",
    "df.drop(['jurisdiction_names', 'market','state','neighbourhood','street','smart_location','host_neighbourhood'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect cols with <=2 unique values\n",
    "inspect = df.loc[:, (df.nunique() <=2)].columns.to_list()\n",
    "\n",
    "#Check\n",
    "display(df[inspect].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary for mapping\n",
    "mapping = {'t':1,'f':0}\n",
    "\n",
    "#Map 1's and 0's on t's and f's\n",
    "df[inspect] = df[inspect].apply(lambda x: x.map(mapping, na_action='ignore'))\n",
    "\n",
    "#Check\n",
    "display(df[inspect].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop columns containing url data or pertain to webscraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset column headers containing 'url' or 'scrape' and store in drop\n",
    "drop = list(df.filter(regex='url|scrape').columns)\n",
    "\n",
    "#Drop drop list and check\n",
    "df.drop(columns= df[drop], inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for high correlations between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create correlation matrix and capture absolute values of correlations\n",
    "c = df.corr().abs()\n",
    "\n",
    "#Create a df that stores correlations between features >.9\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\").reset_index()\n",
    "so.columns = ['feat1','feat2','corr']\n",
    "so = so.loc[ (so.feat1 != so.feat2 )& (so['corr'] > .9)]\n",
    "\n",
    "#Capture list of features\n",
    "feats =so.feat1.unique()\n",
    "\n",
    "#Subset df by cols in feats and create corr\n",
    "corr= df[feats].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fig\n",
    "f, ax = plt.subplots(figsize = (13,13))\n",
    "\n",
    "#Plot corr as heat map\n",
    "sns.heatmap(data = corr, annot=True,fmt='.1%', cmap = 'coolwarm', ax=ax,\n",
    "            linewidths=1.0, square=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop cols with high collinearity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cols with high collinearity\n",
    "drop = ['calculated_host_listings_count_entire_homes','maximum_nights_avg_ntm', 'maximum_maximum_nights','minimum_maximum_nights',\n",
    "        'maximum_minimum_nights','minimum_minimum_nights', 'minimum_nights_avg_ntm', 'host_total_listings_count']\n",
    "\n",
    "#Drop drop\n",
    "df.drop(drop, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up object and numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter cols pertaining to prices and assign col names as a list to money_cols\n",
    "money_cols = df.filter(regex = 'people|deposit|price|fee$|rate').columns.tolist()\n",
    "\n",
    "#Remove $, and set type as numeric for money_cols\n",
    "df[money_cols] = df[money_cols].replace('[$|,|%]','',regex = True).astype('float')\n",
    "\n",
    "#Check\n",
    "display(df[money_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up object columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of columns to apply cleaning to\n",
    "objects = df.select_dtypes('object').columns.to_list()\n",
    "\n",
    "#Check\n",
    "display(df[objects].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove quotes and brackets. Keeping other punctuation\n",
    "df[objects] = df[objects].apply(lambda x : x.str.replace('[{\"}_]',' '))\n",
    "\n",
    "#Check\n",
    "display(df[objects].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import missing_calculator\n",
    "from Missing_Stats import missing_calculator\n",
    "\n",
    "#View missing statistics - datetime64\n",
    "display(missing_calculator(df, data_type='datetime64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is no inherent order within the data and most of the review data is captured, we will leave datetime data as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View missing statistics - float64\n",
    "display(missing_calculator(df, data_type='float64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop columns w/ mostly missing values**\n",
    "\n",
    "Leaving weekly and monthly price since not all hosts have to offer the service and they are sometimes at a special rate compared to the per night rental.\n",
    "\n",
    "Keeping reviews_per_month as is for now, may fill with mean if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping square_feet and host_acceptance_rate\n",
    "df.drop(['square_feet','host_acceptance_rate'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert review scores to categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of cols pertaining to review scores\n",
    "review_cols = df.filter(regex='review_scores').columns.tolist()\n",
    "\n",
    "#Fill review_cols missing values with 'No review data' and convert into category\n",
    "df[review_cols] = df[review_cols].fillna('No review data')\n",
    "df[review_cols] = df[review_cols].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill remaining cols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset cols to apply nafill w/ median\n",
    "cols = ['security_deposit', 'host_response_rate', 'cleaning_fee']\n",
    "\n",
    "#Fill na w/ median\n",
    "df[cols]=df[cols].fillna(df.median().iloc[0])\n",
    "\n",
    "#Subset cols to apply nafill w/ mode\n",
    "cols = ['beds', 'bathrooms', 'bedrooms']\n",
    "\n",
    "#Fill na w/mode\n",
    "df[cols] = df[cols].fillna(df.mode().iloc[0])\n",
    "\n",
    "#Capture cols for filling\n",
    "cols = ['host_has_profile_pic', 'host_identity_verified', 'host_is_superhost', 'host_listings_count' ]\n",
    "\n",
    "#Ffill, bfill remaining missing floats\n",
    "df.loc[:,cols]=df.loc[:,cols].ffill().bfill()\n",
    "\n",
    "#Check\n",
    "display(missing_calculator(df, data_type='float64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store missins stats in missin gdf\n",
    "missing = missing_calculator(df, data_type='object')\n",
    "\n",
    "#View missing statistics - object\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving the follwing text columns alone because they reflect non-mandatory inputs from the host about the living space they are renting out. Likely will not be used for later modeling:\n",
    "\n",
    "* notes\n",
    "* access\n",
    "* interactiion\n",
    "* transit\n",
    "* house_rules, \n",
    "* neighborhood_overview \n",
    "* host_about \n",
    "* space\n",
    "* host_location\n",
    "* host_name\n",
    "* host_neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Miscellaneous Column cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cols for ffill and bfill\n",
    "cols = ['license','host_response_time','cancellation_policy']\n",
    "\n",
    "#Ffill, bfill objects\n",
    "df.loc[:,cols]=df.loc[:,cols].ffill().bfill()\n",
    "\n",
    "df.groupby('is_location_exact')['is_location_exact'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**City clean-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View unique values in city\n",
    "print('Unique values in city:\\n', df.city.unique())\n",
    "\n",
    "#Fix Daly City, San Francisco\n",
    "df.city.replace('[^A-Za-z0-9\\s]','San Francisco',regex = True, inplace=True)\n",
    "df.city.replace('Da.*','Daly City',regex = True, inplace=True)\n",
    "df.city.replace('(San\\sF|Noe|B|Nor).*','San Francisco',regex = True, inplace=True)\n",
    "\n",
    "#Fill na with San Francisco\n",
    "df.city.fillna('San Francisco', inplace=True)\n",
    "\n",
    "#Check\n",
    "print('Unique values in city:\\n', df.city.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zipcode clean-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View unique values in zipcode\n",
    "print('Unique values in zipcode:\\n', df.zipcode.unique())\n",
    "\n",
    "#Remove white spaces and CA\n",
    "df.zipcode.replace('[CA]*\\s*','', regex = True, inplace=True)\n",
    "\n",
    "#Read in libraries to resolve missing values for zipcode using lat/long\n",
    "from uszipcode import SearchEngine\n",
    "from uszipcode import Zipcode\n",
    "\n",
    "#Instantiate SearchEngine\n",
    "zipsearch = SearchEngine(simple_zipcode=True)\n",
    "\n",
    "#Write function that finds zip given lat and long data\n",
    "def get_zipcode(lat, lon):\n",
    "    result = zipsearch.by_coordinates(lat = lat, lng = lon, returns = 1)\n",
    "    return result[0].zipcode\n",
    "\n",
    "#Apply get_zipcode and assign to Zipcode\n",
    "df.zipcode[df.zipcode.isna()]= df[df.zipcode.isna()][['latitude', 'longitude']].swifter.apply(lambda x: get_zipcode(x.latitude, x.longitude), axis =1)\n",
    "\n",
    "#Check\n",
    "print('Unique values in zipcode:\\n', df.zipcode.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Check Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(missing_calculator(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print final shape of df\n",
    "print('Shape of cleaned data:', df.shape)\n",
    "\n",
    "#Set path to local machine\n",
    "path = r'C:\\Users\\kishe\\Documents\\Data Science\\Projects\\Python\\In Progress\\Airbnb - San Francisco\\Data\\02_Cleaned'\n",
    "\n",
    "#Write file\n",
    "df.to_csv(path + '/2020_0520_Listings_Cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA",
   "language": "python",
   "name": "eda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
